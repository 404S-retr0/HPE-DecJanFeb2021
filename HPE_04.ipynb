{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HPE 04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOpiDBvAKqXaY3WWBjiitP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/HPE-DecJanFeb2021/blob/main/HPE_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hKNlZH4KxNJ",
        "outputId": "9613b756-a166-45fc-85a2-6907bd4f4fb6"
      },
      "source": [
        "# NLP\r\n",
        "%tensorflow_version 1.x\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx2D6Pv_RGij",
        "outputId": "c318be1c-e8c9-41f0-9abd-4a317fe9707e"
      },
      "source": [
        "from tensorflow import keras\r\n",
        "imdb = keras.datasets.imdb \r\n",
        "HP_dict_size = 10000 # USAGE -> FREQUENCY of usage \r\n",
        "(xtrain, ytrain),(xtest, ytest) = imdb.load_data(num_words=HP_dict_size)\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/datasets/imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/datasets/imdb.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI8QKXfQZSvB",
        "outputId": "9b24854f-9cde-436b-9339-0429f00423a8"
      },
      "source": [
        "print(ytrain[:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 1 0 0 1 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC6nrWvXZd4c",
        "outputId": "828179a4-2fdd-4dbc-8180-051e34bf1631"
      },
      "source": [
        "wordIndex = imdb.get_word_index()\r\n",
        "print(wordIndex['the'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww3-UtMFZ4rn",
        "outputId": "a0601b74-c6b3-4458-c8e1-f53ffe51a809"
      },
      "source": [
        "print(ytrain[1])\r\n",
        "print(xtrain[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "[1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n3m_52QDaMc_",
        "outputId": "6e567f10-35b9-4aed-ac25-4f9d21513459"
      },
      "source": [
        "dictionary = { encoding:word for word,encoding in wordIndex.items() }\r\n",
        "dictionary[42]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"it's\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "tApIm0uabpAM",
        "outputId": "43bf3cbd-2bba-4be3-919d-2ddabb585aad"
      },
      "source": [
        "# dictionary[0] # word * 0 = 0 \r\n",
        "# NLP-> RULES \r\n",
        "# <PAD> -> 0 \r\n",
        "# <START> -> 1\r\n",
        "# <UNK> -> 2 UNKNOWN-> anything else \r\n",
        "# <UNUSED> -> 3\r\n",
        "\r\n",
        "word_index = { word:(encoding + 3) for word,encoding in wordIndex.items()}\r\n",
        "word_index['<PAD>'] = 0\r\n",
        "word_index['<START>'] = 1\r\n",
        "word_index['<UNK>'] = 2\r\n",
        "word_index['<UNUSED>'] = 3\r\n",
        "\r\n",
        "def decoder(review):\r\n",
        "  decoded_review = [ dictionary.get(word) for word in review ]\r\n",
        "  sentence = ' '.join(decoded_review)\r\n",
        "  return sentence\r\n",
        "\r\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had <UNK> working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how <UNK> this is to watch save yourself an hour a bit of your life\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "LaAf4Olmb1WR",
        "outputId": "120bd9f9-ffdb-4bcb-8005-6941ff2722d7"
      },
      "source": [
        "word_index = { word:(encoding + 3) for word,encoding in wordIndex.items()}\r\n",
        "word_index['<PAD>'] = 0\r\n",
        "word_index['<START>'] = 1\r\n",
        "word_index['<UNK>'] = 2\r\n",
        "word_index['<UNUSED>'] = 3\r\n",
        "dictionary = { encoding:word for word,encoding in word_index.items() }\r\n",
        "def decoder(review):\r\n",
        "  decoded_review = [ dictionary.get(word) for word in review ]\r\n",
        "  sentence = ' '.join(decoded_review)\r\n",
        "  return sentence\r\n",
        "print(ytrain[2])\r\n",
        "decoder(xtrain[2])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had <UNK> working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how <UNK> this is to watch save yourself an hour a bit of your life\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "fS-1QiqZeMjv",
        "outputId": "05e2fdaf-fc27-4cec-8f76-7b80c0ce9c31"
      },
      "source": [
        "print(ytrain[12])\r\n",
        "decoder(xtrain[12])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> i love cheesy horror flicks i don't care if the acting is sub par or whether the monsters look corny i liked this movie except for the <UNK> feeling all the way from the beginning of the film to the very end look i don't need a 10 page <UNK> or a sign with big letters explaining a plot to me but dark floors takes the what is this movie about thing to a whole new annoying level what is this movie about br br this isn't exceptionally scary or thrilling but if you have an hour and a half to kill and or you want to end up feeling frustrated and confused rent this winner\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb-aaft0efnf",
        "outputId": "c7a70acb-0cc7-47e4-e53e-4337b7d8f697"
      },
      "source": [
        "lengths = []\r\n",
        "for x in range(20):\r\n",
        "  lengths.append(len(xtrain[x]))\r\n",
        "print(lengths)\r\n",
        "# INPUT size is NOT of the same SIZE!!!"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[218, 189, 141, 550, 147, 43, 123, 562, 233, 130, 450, 99, 117, 238, 109, 129, 163, 752, 212, 177]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXhcrHrigqcb"
      },
      "source": [
        "# PAD> added. \r\n",
        "# y = mx + c = important word * their weights + empty space*0 + bias \r\n",
        "# y = w1*and + w2* pikachu + w3*run ....   w200*<PAD>= w200*0=0\r\n",
        "xtrain_padded = keras.preprocessing.sequence.pad_sequences(xtrain, value=0, \r\n",
        "                                                           padding='post', \r\n",
        "                                                           truncating='post',\r\n",
        "                                                           maxlen=256)\r\n",
        "xtest_padded = keras.preprocessing.sequence.pad_sequences(xtest, value=0, \r\n",
        "                                                           padding='post', \r\n",
        "                                                           truncating='post',\r\n",
        "                                                           maxlen=256)\r\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "FoKkbJj8h4o8",
        "outputId": "68f0a5ad-74a7-4a14-dcc7-af6492109f61"
      },
      "source": [
        "decoder(xtrain_padded[2])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had <UNK> working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how <UNK> this is to watch save yourself an hour a bit of your life <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jfgtCLCh9qv",
        "outputId": "f091d56b-1951-4b4d-efa4-2562f2706647"
      },
      "source": [
        "xtrain_padded[2]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,   14,   47,    8,   30,   31,    7,    4,  249,  108,    7,\n",
              "          4, 5974,   54,   61,  369,   13,   71,  149,   14,   22,  112,\n",
              "          4, 2401,  311,   12,   16, 3711,   33,   75,   43, 1829,  296,\n",
              "          4,   86,  320,   35,  534,   19,  263, 4821, 1301,    4, 1873,\n",
              "         33,   89,   78,   12,   66,   16,    4,  360,    7,    4,   58,\n",
              "        316,  334,   11,    4, 1716,   43,  645,  662,    8,  257,   85,\n",
              "       1200,   42, 1228, 2578,   83,   68, 3912,   15,   36,  165, 1539,\n",
              "        278,   36,   69,    2,  780,    8,  106,   14, 6905, 1338,   18,\n",
              "          6,   22,   12,  215,   28,  610,   40,    6,   87,  326,   23,\n",
              "       2300,   21,   23,   22,   12,  272,   40,   57,   31,   11,    4,\n",
              "         22,   47,    6, 2307,   51,    9,  170,   23,  595,  116,  595,\n",
              "       1352,   13,  191,   79,  638,   89,    2,   14,    9,    8,  106,\n",
              "        607,  624,   35,  534,    6,  227,    7,  129,  113,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "bM10p17niIJ3",
        "outputId": "c48be6dd-749d-42d4-d06c-fd94c97cdb06"
      },
      "source": [
        "decoder(xtrain_padded[7])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> the <UNK> tells the story of the four hamilton siblings teenager francis <UNK> <UNK> twins <UNK> joseph <UNK> <UNK> <UNK> <UNK> the <UNK> david samuel who is now the surrogate parent in charge the <UNK> move house a lot <UNK> is unsure why is unhappy with the way things are the fact that his brother's sister kidnap <UNK> murder people in the basement doesn't help relax or calm <UNK> nerves either francis <UNK> something just isn't right when he eventually finds out the truth things will never be the same again br br co written co produced directed by mitchell <UNK> phil <UNK> as the butcher brothers who's only other film director's credit so far is the april <UNK> day 2008 remake enough said this was one of the <UNK> to die <UNK> at the 2006 after dark <UNK> or whatever it's called in keeping with pretty much all the other's i've seen i thought the <UNK> was complete total utter crap i found the character's really poor very unlikable the slow moving story failed to capture my imagination or sustain my interest over it's 85 a half minute too long <UNK> minute duration the there's the awful twist at the end which had me laughing out loud there's this really big <UNK> build up to what's inside a <UNK> thing in the <UNK> basement it's eventually revealed to be a little boy with a teddy is that really supposed to scare us is that really supposed to shock us is that really something that\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZIVDQ3ZiU-C"
      },
      "source": [
        "HP_dict_size = 10000\r\n",
        "HP_epochs = 30\r\n",
        "HP_embedded_dims = 16\r\n",
        "HP_maxlen = 256\r\n",
        "HP_val_size = 10000\r\n",
        "HP_batch_size = 128  # constant for models\r\n",
        "\r\n",
        "HP_d_l3_l4_m1 = 64    # DENSER network\r\n",
        "HP_d_l4_l5_m1 = 128\r\n",
        "HP_d_l3_l4_m2 = 32     # shallower network \r\n",
        "HP_d_l4_l5_m2 = 64\r\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ10onydv7Zv"
      },
      "source": [
        "# Model 1 layers\r\n",
        "l1_m1 = keras.layers.Embedding(HP_dict_size, HP_embedded_dims)\r\n",
        "l2_m1 = keras.layers.GlobalAveragePooling1D()\r\n",
        "l3_m1 = keras.layers.Dense(HP_d_l3_l4_m1)\r\n",
        "l4_m1 = keras.layers.Dense(HP_d_l4_l5_m1, activation=tf.nn.relu)\r\n",
        "l5_m1 = keras.layers.Dense(1, activation=tf.nn.sigmoid)\r\n",
        "layers_m1 = [l1_m1,l2_m1,l3_m1,l4_m1,l5_m1]\r\n",
        "# Model 2 layers\r\n",
        "l1_m2 = keras.layers.Embedding(HP_dict_size, HP_embedded_dims)\r\n",
        "l2_m2 = keras.layers.GlobalAveragePooling1D()\r\n",
        "l3_m2 = keras.layers.Dense(HP_d_l3_l4_m2)\r\n",
        "l4_m2 = keras.layers.Dense(HP_d_l4_l5_m2, activation=tf.nn.relu)\r\n",
        "l5_m2 = keras.layers.Dense(1, activation=tf.nn.sigmoid)\r\n",
        "layers_m2 = [l1_m2,l2_m2,l3_m2,l4_m2,l5_m2]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIenxu44w8_k",
        "outputId": "72cb980c-f198-4fc2-a823-2c0b30251d3b"
      },
      "source": [
        "m1= keras.Sequential(layers_m1)\r\n",
        "m2 = keras.Sequential(layers_m2)\r\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkQFCxD0xub0",
        "outputId": "7bfa8414-47b2-4189-b039-dad3de466dbc"
      },
      "source": [
        "m1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\r\n",
        "m2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roBn4edPyOMR",
        "outputId": "596f7c9e-1db7-4127-96f6-55588de88a0a"
      },
      "source": [
        "m1.summary()\r\n",
        "# i1v1 + i2v2 ... i160000v1600000\r\n",
        "# v-> dimensions\r\n",
        "# i -> weights\r\n",
        "\r\n",
        "# DENSE-> "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_2 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                1088      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 169,537\n",
            "Trainable params: 169,537\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PCkE7zuyT2_",
        "outputId": "470dcebc-625d-49bc-f2f7-16955573371d"
      },
      "source": [
        "m2.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_3 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 162,721\n",
            "Trainable params: 162,721\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccm82hq20Wkm"
      },
      "source": [
        "\r\n",
        "xval = xtest_padded[:HP_val_size]\r\n",
        "yval = ytest[:HP_val_size]\r\n",
        "ytest_red = ytest[HP_val_size:]\r\n",
        "xtest_red = xtest_padded[HP_val_size:]\r\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvve7aZX06PX"
      },
      "source": [
        "import time \r\n",
        "start_time= time.time()\r\n",
        "# Deep Learning here\r\n",
        "\r\n",
        "history = m1.fit(xtrain_padded, ytrain, epochs=HP_epochs,\r\n",
        "                 batch_size=HP_batch_size,\r\n",
        "                 validation_data=(xval,yval),\r\n",
        "                 verbose=0)\r\n",
        "\r\n",
        "# Deep Learning Ends\r\n",
        "end_time = time.time()\r\n",
        "time_taken = end_time - start_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRy0Cu_o1mQZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}